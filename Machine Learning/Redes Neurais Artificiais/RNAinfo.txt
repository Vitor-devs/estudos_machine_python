Muito bom para muitos dados e problemas complexos 
Visa imitar o sistema nervoso de humanos no processo de aprendizagem
Neuronio
Dendrito - Por onde entra por um valor de entrada e o neuronio ativa dependendo do valor de entrada - 
Axonio- Onde percorre o sinal eletrico - faz a ligação entre neuronios
O resultado depende do que sair, isto é, se for ativado

Neuronio Artificial
Composto por:
Entrada (N numero de coisa) - X
Peso (Um peso para cada entrada) - W
Entra as entrada com o peso na função de somatório somando o (xi * wi) + (xi * wi)
Em seguida entra na step function/função degrau (Que diz se ela é ativada ou não)


Ex:
Maior que zero - ativa
Caso contrario - não ativa

(Sinapse)
Peso positivo - Aumenta a função degrau
Peso negativo - Inibe a função degrau
Conhecimento da rede neural são os pesos, então as entradas podem ser as mesmas mas se mudar os pesos tudo muda
Analisa os atributos e decide os pesos

Ex:                                                 w1         w2      w3      w4            w5
Achar peso para cada um dos atributos (entradas: Historia, salario, divida, garantia e renda anual) entra a função de soma + ativação e ve o que sai
Treinamento - Achar pesos
E achar o atributo classe

aprendizagem por reforço - Aprender com as interações
Focando em classificação

Dicas:
Para encontrar bons pesos se usa o Operador AND nos atributos previsores
Se treina a rede neural para conseguir encontrar 

Os pesos são atualizados até os erros serem pequenos

peso(n+1) = peso(n) + (taxaAprendizagem*entrada*erro)
Onde n = valor atual do peso

É usado o mesmo peso para cada atributo
erro = resposta correta - resposta obtida

Quando não for possível trabalhar com um problema linearmente separável, então se usa multicamadas (Cada entrada se comunica com as camadas (Cada = Soma + Ativação)) e elas falam com a camada de saída
De cada lado tem pesos
Estradas (com pesos) - Se liga a n camadas - cada n camada se liga com a camada de saída

A rede processa e retorna uma resposta(Feed forward)

Step function pode ser a sigmoide tbm - y = 1 / (1 + e-^x)
se x for grande é mais proximo de um 
Tem a tangente hiperbólica
Porém existe varias outras funções de ativação


REDES MULTICAMADAS ------------------------------------------------
2 entradas com 3 camadas
Para cada entrada se faz a multiplicação da entrada * peso
no fim, faz a soma. Depois se faz a conta para o valor de ativação (Sigmoide por ex, sendo o resultado da soma o valor x da função sigmoide (y = 1 / (1 + e-^x))) - Então para cada resultado da camada, se faz a soma, e então é aplicado a função de ativação 

Fazendo isso para cada atributo previsor
Depois disso se faz a conta do erro (erro = resposta esperada - resposta obtida )
Sempre se visa diminuir o erro ou a média do erro
1. Inicia os pesos com valores aleatórios
2. Baseado nos dados (Aprendizagem Supervisionada), realiza os cálculos com os pesos e calcula o erro
3. Calcula as mudanças nos pesos e os atualiza (Backpropagation/Retropropragação)
4. O algoritmo termina quando o erro é pequeno

Inicializa pesos => Calcula saídas => Calcula o erro => Calcula pesos => Atualiza pesos => (Verifica se o erro é pequeno ou não) se não ele recalcula pesos

Cost function = a quantidade de erro que a rede neural tem 

Gradiente
Imagina uma rede, em um plano tridimensional, seja la qual for, ele tende a ir descendo, até parar na base, o que é entendivel, e com cada peso ele tem que ir descendo até o erro estar la embaixo (Visa o minimo global) - Bolinha de gude em uma tijela
Meios:
Derivada }=> da curva 
Cálculo do delta }=> auxilia no gradiente
Backpropagation }
Learning Rate
Momentum
Como? 
-força bruta (testando todas) 
-simmulated anealing
-algoritmos genéticos
Gradiente é calculado para saber quanto ajustar os pesos
Grafico de erro x peso
Direcionamos o algoritmo para alcançar o maior fundo
Para saber onde ele vai?
pega o resultado da sigmoide e joga na formula d = y * (1-y)

Para onde ir? Usa o delta da camada de saida para cada camada
DeltaSaida = Erro * DerivadaSigmoide 

isso para cada neuronio (o delta de saida é o mesmo para todos) 
DeltaEscondida = DerivadaSigmoide (o que vem da função de ativação) * peso * DeltaSaida

(Fit forward)
Entrada => Camada Escondida => Camada de Saída

(Back propagation)
Camada de Saída => Camada Escondida => Entrada
peso    = (peso * momento) + (entrada (peso) * delta * taxa de aprendizagem)
    (n+1)
valor de cada função sigmoide * delta de saida } para cada entrada ~ para cada neuronio
dai se soma tudo } desse valor para cada neuronio
usando a fórmula acima, substitui cada valor na parte de (entrada * delta)
acha-se assim, o peso, com que faz com que os antigos valores de peso sejam mudados

momento = Escapar de Minimos Locais (nem sempre funciona) - definido pelo usuario
    Define o quão confiável é a ultima alteração 
        Alto: Aumenta a velocidade de convergência
        Baixo: Pode evitar mínimos locais

taxa de aprendizagem = define o quão rápido o algoritmo vai aprender - definido pelo usuario
    Alto: A convergência é rápida mas pode perder o mínimo global (O minimo erro)
    Baixo: Será mais lento mas tem mais chances de chegar no mínimo global (O minimo erro)

Bias (viés) - ele adiciona uma entrada a mais com um peso especifico 

Erros:
MSE - MEAN SQUARE ERROR => calcula todos os erros, eleva ao quadrado => soma os resultados => divide a soma pelo numero de registros => resultado é o erro
RMSE - ROOT MEAN SQUARE ERROR => mesma coisa só que tira a raiz 

Calcula a média da diferença entre o esperado e o que foi previsto pela redeerros maiores contam mais que erros menores
Penaliza erros maiores

Quando se precisa classificar mais saidas é normal ter mais de um neuronio de saida

Deep Learning - Rede neural profunda (duas ou mais camadas)
entrada > 2 ou mais camadas > saida
- Gradiente vai ficando menor, logo fica muito pequeno as mudanças
- Se pode usar outros
Por onde seguir no deep learning? redes neurais convolucionais - recorrentes
Keras, Theano, TensorFlow